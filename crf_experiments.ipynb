{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle\n",
    "import argparse\n",
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import scorers\n",
    "from sklearn_crfsuite import metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from os.path import join, split, splitext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"medium\"\n",
    "model = \"medium\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"loading dataset....\")\n",
    "train = pd.read_csv(\"../train_{}.csv\".format(dataset),\n",
    "                    usecols=[\"document_type\", \"pages\", \"process_id\"])\n",
    "valid = pd.read_csv(\"../validation_{}.csv\".format(dataset),\n",
    "                    usecols=[\"document_type\", \"pages\", \"process_id\"])\n",
    "test = pd.read_csv(\"../test_parts_{}.csv\".format(dataset),\n",
    "                   usecols=[\"document_type\", \"pages\", \"process_id\"])\n",
    "train[\"document_type\"] = train.apply(lambda x: \"B-\" + x[\"document_type\"] if x[\"pages\"] == 1 else \"I-\" + x[\"document_type\"],\n",
    "                                     axis=1)\n",
    "valid[\"document_type\"] = valid.apply(lambda x: \"B-\" + x[\"document_type\"] if x[\"pages\"] == 1 else \"I-\" + x[\"document_type\"],\n",
    "                                     axis=1)\n",
    "test[\"document_type\"] = test.apply(lambda x: \"B-\" + x[\"document_type\"] if x[\"pages\"] == 1 else \"I-\" + x[\"document_type\"],\n",
    "                                   axis=1)\n",
    "\n",
    "\n",
    "with open(\"../document_vectors_{}_{}.pkl\".format(model, dataset), \"rb\") as file:\n",
    "    vectors = pickle.load(file)\n",
    "X_train = vectors[\"train_vectors\"]\n",
    "X_valid = vectors[\"valid_vectors\"]\n",
    "X_test = vectors[\"test_vectors\"]\n",
    "y_train = train[\"document_type\"].tolist()\n",
    "y_valid = valid[\"document_type\"].tolist()\n",
    "y_test = test[\"document_type\"].tolist()\n",
    "print(\"....done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_to_process(data, vectors):\n",
    "    xs = []\n",
    "    ys = []\n",
    "    data[\"data\"] = vectors.tolist()\n",
    "    for k, v in data.groupby(\"process_id\").groups.items():\n",
    "        xs.append(data.iloc[v][\"data\"].tolist())\n",
    "        ys.append(data.iloc[v][\"document_type\"].tolist())\n",
    "    return xs, ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Converting data to sentences...\")\n",
    "X_train, y_train = data_to_process(train, X_train)\n",
    "X_valid, y_valid = data_to_process(valid, X_valid)\n",
    "X_test, y_test = data_to_process(test, X_test)\n",
    "print(\"...done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_train), len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data2feat(data):\n",
    "    feat_data = []\n",
    "    for i, sentence in enumerate(data):\n",
    "        feat_data.append([])\n",
    "        for j, token in enumerate(sentence):\n",
    "            feat_data[i].append({ str(i) : d for i, d in enumerate(token)})\n",
    "    return feat_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data2feat(X_train)\n",
    "X_valid = data2feat(X_valid)\n",
    "X_test = data2feat(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crf = sklearn_crfsuite.CRF(\n",
    "    verbose=True,\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.1,\n",
    "    c2=0.1,\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True,\n",
    "    all_possible_states=True\n",
    ")\n",
    "crf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = crf.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group B and I results\n",
    "sorted_labels = sorted(\n",
    "    labels,\n",
    "    key=lambda name: (name[1:], name[0])\n",
    ")\n",
    "y_pred = crf.predict(X_test)\n",
    "print(metrics.flat_classification_report(\n",
    "    y_test, y_pred, labels=sorted_labels, digits=3\n",
    "))\n",
    "print(metrics.flat_accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.grid_search import RandomizedSearchCV\n",
    "\n",
    "\n",
    "# define fixed parameters and parameters to search\n",
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True,\n",
    "    all_possible_states=True\n",
    ")\n",
    "params_space = {\n",
    "    'c1': scipy.stats.expon(scale=0.5),\n",
    "    'c2': scipy.stats.expon(scale=0.05),\n",
    "}\n",
    "\n",
    "# use the same metric for evaluation\n",
    "f1_scorer = make_scorer(metrics.flat_f1_score,\n",
    "                        average='weighted', labels=labels)\n",
    "\n",
    "# search\n",
    "rs = RandomizedSearchCV(crf, params_space,\n",
    "                        cv=3,\n",
    "                        verbose=1,\n",
    "                        n_jobs=-1,\n",
    "                        n_iter=50,\n",
    "                        scoring=f1_scorer)\n",
    "rs.fit(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crf = rs.best_estimator_\n",
    "print('best params:', rs.best_params_)\n",
    "print('best CV score:', rs.best_score_)\n",
    "print('model size: {:0.2f}M'.format(rs.best_estimator_.size_ / 1000000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "\n",
    "_x = [s.parameters['c1'] for s in rs.grid_scores_]\n",
    "_y = [s.parameters['c2'] for s in rs.grid_scores_]\n",
    "_c = [s.mean_validation_score for s in rs.grid_scores_]\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.set_size_inches(12, 12)\n",
    "ax = plt.gca()\n",
    "ax.set_yscale('log')\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlabel('C1')\n",
    "ax.set_ylabel('C2')\n",
    "ax.set_title(\"Randomized Hyperparameter Search CV Results (min={:0.3}, max={:0.3})\".format(\n",
    "    min(_c), max(_c)\n",
    "))\n",
    "\n",
    "ax.scatter(_x, _y, c=_c, s=60, alpha=0.9, edgecolors=[0,0,0])\n",
    "\n",
    "print(\"Dark blue => {:0.4}, dark red => {:0.4}\".format(min(_c), max(_c)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crf = sklearn_crfsuite.CRF(\n",
    "    verbose=True,\n",
    "    algorithm='lbfgs',\n",
    "    c1=rs.best_params_['c1'],\n",
    "    c2=rs.best_params_['c2'],\n",
    "    max_iterations=1000,\n",
    "    all_possible_transitions=True,\n",
    "    all_possible_states=True\n",
    ")\n",
    "crf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group B and I results\n",
    "sorted_labels = sorted(\n",
    "    labels,\n",
    "    key=lambda name: (name[1:], name[0])\n",
    ")\n",
    "y_pred = crf.predict(X_test)\n",
    "print(metrics.flat_classification_report(\n",
    "    y_test, y_pred, labels=sorted_labels, digits=3\n",
    "))\n",
    "print(metrics.flat_accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_class = []\n",
    "y_test_class = []\n",
    "\n",
    "for i, sequence in enumerate(y_pred):\n",
    "    y_pred_class.append([])\n",
    "    for j, pred in enumerate(sequence):\n",
    "        y_pred_class[i].append(pred[2:])\n",
    "        \n",
    "for i, sequence in enumerate(y_test):\n",
    "    y_test_class.append([])\n",
    "    for j, pred in enumerate(sequence):\n",
    "        y_test_class[i].append(pred[2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_class[0][0], y_test_class[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_labels=['acordao_de_2_instancia', 'agravo_em_recurso_extraordinario',\n",
    "                'despacho_de_admissibilidade', 'outros', 'peticao_do_RE', 'sentenca']\n",
    "\n",
    "print(metrics.flat_classification_report(\n",
    "    y_test_class, y_pred_class, labels=sorted_labels, digits=3\n",
    "))\n",
    "print(metrics.flat_accuracy_score(y_test_class, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"models/crf_{}_{}\".format(model, dataset), \"wb\") as file:\n",
    "    pickle.dump(crf, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def print_transitions(trans_features):\n",
    "    for (label_from, label_to), weight in trans_features:\n",
    "        print(\"%-6s -> %-7s %0.6f\" % (label_from, label_to, weight))\n",
    "\n",
    "print(\"Top likely transitions:\")\n",
    "print_transitions(Counter(crf.transition_features_).most_common(20))\n",
    "\n",
    "print(\"\\nTop unlikely transitions:\")\n",
    "print_transitions(Counter(crf.transition_features_).most_common()[-20:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_state_features(state_features):\n",
    "    for (attr, label), weight in state_features:\n",
    "        print(\"%0.6f %-8s %s\" % (weight, label, sorted_labels[int(attr)]))\n",
    "\n",
    "print(\"Top positive:\")\n",
    "print_state_features(Counter(crf.state_features_).most_common(30))\n",
    "\n",
    "print(\"\\nTop negative:\")\n",
    "print_state_features(Counter(crf.state_features_).most_common()[-30:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
